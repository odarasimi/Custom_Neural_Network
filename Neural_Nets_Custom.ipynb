{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Nets_Custom",
      "provenance": [],
      "authorship_tag": "ABX9TyOTwm3Pw0M25Thpyh65llFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odarasimi/Custom_Neural_Network/blob/master/Neural_Nets_Custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUsbZabs8HXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from statistics import mean"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEaqGLC2gx24",
        "colab_type": "text"
      },
      "source": [
        "**Defining the loss function, prediction function and gradient descent function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d0_jEVx0eYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_loss(y_true, y_predicted):\n",
        "  y_predicted_new = [max(i,epsilon for i in y_predicted)]\n",
        "  y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
        "  y_predicted_new = np.array(y_predicted_new)\n",
        "  return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbjV3LsNRVJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_np(X):\n",
        "   return 1/(1+np.exp(-X))\n",
        "\n",
        "def prediction_function(w1, w2, b, value1, value2):\n",
        "    weighted_sum = w1*value1 + w2*value2 + b\n",
        "    return sigmoid_np(weighted_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHFvMuB4kRvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(Var1, Var2, y_true, epochs, loss_threshold):\n",
        "    w1 = w2 = 1\n",
        "    bias = 0\n",
        "    rate = 0.5\n",
        "    n = len(Var1)\n",
        "    for i in range(epochs):\n",
        "        y_predicted = prediction_function(w1, w2, b, Var1, Var2)\n",
        "        loss = log_loss(y_true, y_predicted)\n",
        "\n",
        "        w1d = (1/n)*np.dot(np.transpose(Var1),(y_predicted-y_true)) \n",
        "        w2d = (1/n)*np.dot(np.transpose(Var2),(y_predicted-y_true)) \n",
        "        bias_d = np.mean(y_predicted-y_true)\n",
        "\n",
        "        w1 = w1 - rate * w1d\n",
        "        w2 = w2 - rate * w2d\n",
        "        bias = bias - rate * bias_d\n",
        "\n",
        "        print (f'Epoch:{i}, w1:{w1}, w2:{w2}, Bias:{bias}, Loss:{loss}')\n",
        "\n",
        "        if loss<=loss_threshold:\n",
        "            break\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svcQfXo7griw",
        "colab_type": "text"
      },
      "source": [
        "**Implementing the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G19buwTPdu-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class scratch_nn:\n",
        "  def __init__(self, w1 = 1, w2 = 1, bias = 0):\n",
        "    self.w1 = w1\n",
        "    self.w2 = w2\n",
        "    self.bias = bias\n",
        "  \n",
        "  def fit(self, X, y, epochs, loss_threshold):\n",
        "    self.w1, self.w2, self.bias = self.gradient_descent(X[\"Var1\"], X[\"Var2\"], y, epochs, loss_threshold)\n",
        "\n",
        "  def gradient_descent(Var1, Var2, y_true, epochs, loss_threshold):\n",
        "    w1 = self.w1\n",
        "    w2 = self.w2\n",
        "    bias = self.bias\n",
        "    learning_rate = 0.5\n",
        "    n = len(Var1)\n",
        "    for i in range(epochs):\n",
        "        y_predicted = prediction_function(w1, w2, bias, Var1, Var2)\n",
        "        loss = log_loss(y_true, y_predicted)\n",
        "\n",
        "        w1_d = (1/n)*np.dot(np.transpose(Var1),(y_predicted-y_true)) \n",
        "        w2_d = (1/n)*np.dot(np.transpose(Var2),(y_predicted-y_true)) \n",
        "        bias_d = np.mean(y_predicted-y_true)\n",
        "\n",
        "        w1 = w1 - learning_rate * w1_d\n",
        "        w2 = w2 - learning_rate * w2_d\n",
        "        bias = bias - learning_rate * bias_d\n",
        "\n",
        "        if loss<=loss_threshold:\n",
        "            break\n",
        "    return w1, w2, bias\n",
        "\n",
        "    def predict(self, X):\n",
        "      return (sigmoid_np(self.w1*X[\"age\"] + self.w2*X[affordability] + self.bias))\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}